{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WijzJUfcbzmV"
   },
   "source": [
    "# Training of conditional generators\n",
    "\n",
    "This notebook trains and experiments with GPT2-based conditional text generators using different configurations: labeling strategies, loss functions, architectures, ...\n",
    "\n",
    "We use two types of labels to condition the generation: form and topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `run_as_standalone_nb = True` if you are running this notebook outside of a clone of its repository (https://github.com/Poems-AI/AI.git). For example, in a Colab or Kaggle notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_as_standalone_nb = True\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "if run_as_standalone_nb:\n",
    "    import sys    \n",
    "    root_lib_path = Path('AI').resolve()\n",
    "    if not root_lib_path.exists():\n",
    "        !git clone https://github.com/Poems-AI/AI.git\n",
    "    if str(root_lib_path) not in sys.path:\n",
    "        sys.path.insert(0, str(root_lib_path))\n",
    "\n",
    "    !pip install -r {root_lib_path/'requirements.txt'}\n",
    "    !apt-get install git-lfs\n",
    "    !git lfs install\n",
    "else:\n",
    "    import local_lib_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:33.090380Z",
     "iopub.status.busy": "2022-04-04T21:37:33.090057Z",
     "iopub.status.idle": "2022-04-04T21:37:36.549380Z",
     "shell.execute_reply": "2022-04-04T21:37:36.548271Z",
     "shell.execute_reply.started": "2022-04-04T21:37:33.090347Z"
    },
    "id": "W6y8saYIIMjj"
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict, load_dataset, load_metric\n",
    "from enum import auto, Enum\n",
    "from functools import partial\n",
    "from happytransformer import fine_tuning_util, GENEvalArgs, GENSettings, GENTrainArgs, HappyGeneration\n",
    "from huggingface_hub import login, notebook_login\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import tempfile\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, AutoTokenizer, default_data_collator, Trainer, TrainingArguments\n",
    ")\n",
    "from transformers.optimization import SchedulerType\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:37.524750Z",
     "iopub.status.busy": "2022-04-04T21:37:37.523552Z",
     "iopub.status.idle": "2022-04-04T21:37:38.167936Z",
     "shell.execute_reply": "2022-04-04T21:37:38.166981Z",
     "shell.execute_reply.started": "2022-04-04T21:37:37.524689Z"
    }
   },
   "outputs": [],
   "source": [
    "from poemsai.data import LabelsType, LabelsDecoderExplained, Lang, lang_to_str, PoemsFileConfig\n",
    "from poemsai.hf_utils import model_to_url\n",
    "from poemsai.metrics import (\n",
    "    ConditionalGenLoss, get_compute_metrics_metadataless, MetadataLessLoss, \n",
    "    preprocess_logits_for_metadataless_loss\n",
    ")\n",
    "from poemsai.nb_utils import commit_checkpoint_to_hf_hub, download_checkpoint_from_hf_hub\n",
    "from poemsai.tokenization import add_special_token\n",
    "from poemsai.trainer import PoemsTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone our datasets repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:38.170517Z",
     "iopub.status.busy": "2022-04-04T21:37:38.170196Z",
     "iopub.status.idle": "2022-04-04T21:37:38.940785Z",
     "shell.execute_reply": "2022-04-04T21:37:38.939671Z",
     "shell.execute_reply.started": "2022-04-04T21:37:38.170472Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Poems-AI/dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log in to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:38.943517Z",
     "iopub.status.busy": "2022-04-04T21:37:38.943181Z",
     "iopub.status.idle": "2022-04-04T21:37:38.949085Z",
     "shell.execute_reply": "2022-04-04T21:37:38.948041Z",
     "shell.execute_reply.started": "2022-04-04T21:37:38.943467Z"
    }
   },
   "outputs": [],
   "source": [
    "HF_USER = \"YOUR_HUGGINGFACE_USER\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1: notebook_login.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: get token.** Unfortunately, you need to manually set your password. Every time you push to hub, you'll need to pass `use_auth_token=login_token`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = ''\n",
    "login_token = login(HF_USER, pwd)\n",
    "pwd = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3 (recommended): interact with the git repo that stores your model** and pass the password every time you commit\n",
    "<br><br>\n",
    "Before commiting, you need to tell git your user and email (from HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:38.951908Z",
     "iopub.status.busy": "2022-04-04T21:37:38.951159Z",
     "iopub.status.idle": "2022-04-04T21:37:40.478310Z",
     "shell.execute_reply": "2022-04-04T21:37:40.477132Z",
     "shell.execute_reply.started": "2022-04-04T21:37:38.951840Z"
    }
   },
   "outputs": [],
   "source": [
    "HF_EMAIL = \"YOUR_HUGGINGFACE_EMAIL\"\n",
    "!git config --global user.email $HF_EMAIL\n",
    "!git config --global user.name $HF_USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can push to hub by calling `commit_checkpoint_to_hub`. For instance:\n",
    "```\n",
    "commit_checkpoint_to_hub('gpt2-poems.en', HF_USER, './checkpoints/checkpoint-7170', \n",
    "                         message='Update model after 50 epochs', pwd='your_hf_password')\n",
    "```\n",
    "Be aware that this will copy everything from the checkpoint to the repository. If you need more control,\n",
    "clone the model repository and interact with it like with any other GitHub repository. You can get the url of the\n",
    "model repository with `model_to_url`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, our conditional generators only work with English poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:40.483195Z",
     "iopub.status.busy": "2022-04-04T21:37:40.482824Z",
     "iopub.status.idle": "2022-04-04T21:37:40.488726Z",
     "shell.execute_reply": "2022-04-04T21:37:40.487563Z",
     "shell.execute_reply.started": "2022-04-04T21:37:40.483158Z"
    }
   },
   "outputs": [],
   "source": [
    "lang = Lang.English\n",
    "lang_str = lang_to_str(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:40.491360Z",
     "iopub.status.busy": "2022-04-04T21:37:40.490360Z",
     "iopub.status.idle": "2022-04-04T21:37:40.500893Z",
     "shell.execute_reply": "2022-04-04T21:37:40.499674Z",
     "shell.execute_reply.started": "2022-04-04T21:37:40.491310Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_datasets(txt_paths, tokenizer, num_procs=1):\n",
    "    train_path, eval_path = txt_paths\n",
    "    dataset = load_dataset(\"text\", data_files={\"train\": train_path, \"eval\": eval_path})\n",
    "    tokenized_dataset = fine_tuning_util.preprocess_concatenate(tokenizer, dataset, num_procs, mlm=False)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:40.504299Z",
     "iopub.status.busy": "2022-04-04T21:37:40.502912Z",
     "iopub.status.idle": "2022-04-04T21:37:40.514724Z",
     "shell.execute_reply": "2022-04-04T21:37:40.513449Z",
     "shell.execute_reply.started": "2022-04-04T21:37:40.504249Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_tokenizer(tokenizer, file_config, model):\n",
    "    bov_token_id, eov_token_id, eop_token_id = None, None, None\n",
    "    \n",
    "    if file_config.beginning_of_verse_token != '':\n",
    "        bov_token_id = add_special_token(file_config.beginning_of_verse_token,\n",
    "                                         tokenizer,\n",
    "                                         model,\n",
    "                                         copy_from='')    \n",
    "    if file_config.end_of_verse_token != '':\n",
    "        eov_token_id = add_special_token(file_config.end_of_verse_token,\n",
    "                                         tokenizer,\n",
    "                                         model,\n",
    "                                         copy_from='\\n')\n",
    "    if file_config.end_of_poem_token != '':\n",
    "        eop_token_id = add_special_token(file_config.end_of_poem_token,\n",
    "                                         tokenizer,\n",
    "                                         model,\n",
    "                                         copy_from=tokenizer.eos_token)\n",
    "        \n",
    "    return bov_token_id, eov_token_id, eop_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:40.531592Z",
     "iopub.status.busy": "2022-04-04T21:37:40.530898Z",
     "iopub.status.idle": "2022-04-04T21:37:41.311088Z",
     "shell.execute_reply": "2022-04-04T21:37:41.309939Z",
     "shell.execute_reply.started": "2022-04-04T21:37:40.531545Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:41.313903Z",
     "iopub.status.busy": "2022-04-04T21:37:41.313270Z",
     "iopub.status.idle": "2022-04-04T21:37:41.327413Z",
     "shell.execute_reply": "2022-04-04T21:37:41.326065Z",
     "shell.execute_reply.started": "2022-04-04T21:37:41.313839Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_trainer(model, tokenizer, datasets, n_epochs, bs=1, output_path='./checkpoints', optimizers=(None, None),\n",
    "                  compute_metrics=None, preprocess_logits_for_metrics=None, extra_loss_fns=None, callbacks=None,\n",
    "                  **train_kwargs):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_path,\n",
    "        evaluation_strategy=transformers.trainer_utils.IntervalStrategy.EPOCH, # STEPS,\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs,\n",
    "        num_train_epochs=n_epochs,\n",
    "        save_strategy=transformers.trainer_utils.IntervalStrategy.EPOCH, # defaults to \"steps\"  \n",
    "        report_to=[\"none\"],\n",
    "        **train_kwargs\n",
    "    )\n",
    "    trainer = PoemsTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets['train'],\n",
    "        eval_dataset=datasets['eval'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        #data_collator_ = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        optimizers=optimizers,\n",
    "        compute_metrics=compute_metrics,\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "        extra_loss_fns=extra_loss_fns,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:41.332484Z",
     "iopub.status.busy": "2022-04-04T21:37:41.332079Z",
     "iopub.status.idle": "2022-04-04T21:37:41.342961Z",
     "shell.execute_reply": "2022-04-04T21:37:41.341820Z",
     "shell.execute_reply.started": "2022-04-04T21:37:41.332420Z"
    }
   },
   "outputs": [],
   "source": [
    "orig_model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr 1: train with label inserted as an additional verse at the beginning of the poem\n",
    "\n",
    "Some poems have only topic as label and the rest have only form as label, so we train a different model for each subset of poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_type = LabelsType.Topics\n",
    "labels_type_str = labels_type.value[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_{labels_type_str}.train.{lang_str}.txt\"\n",
    "valid_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_{labels_type_str}.valid.{lang_str}.txt\"\n",
    "custom_model_name = f\"gpt2-poems-cond-{labels_type_str}.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HUGGING_FACE_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen = HappyGeneration(\"GPT2\", orig_model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(orig_model_name, resid_pdrop=0.3)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(orig_model_name)\n",
    "model = happy_gen.model\n",
    "tokenizer = happy_gen.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = PoemsFileConfig.from_json(Path(train_text_path).parent/'all_poems.en.conf.json')\n",
    "begin_verse_id, end_verse_id, end_poem_id = setup_tokenizer(tokenizer, \n",
    "                                                            file_config, \n",
    "                                                            model)\n",
    "(tokenizer.additional_special_tokens, \n",
    " tokenizer.additional_special_tokens_ids, \n",
    " tokenizer.all_special_tokens, \n",
    " begin_verse_id, end_verse_id, end_poem_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets((train_text_path, valid_text_path), tokenizer, num_procs=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False\n",
    "if debug_mode:\n",
    "    from datasets import DatasetDict\n",
    "    datasets = DatasetDict({k: v.select([1, 2, 3]) for k, v in datasets.items()})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of the decoding of a an already tokenized sample. If everything is ok, '\\\\n' should appear at the end of each verse and '<|endoftext|>' at the end of each poem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "n_steps_by_epoch = len(datasets['train']) // bs\n",
    "compute_metrics = get_compute_metrics_metadataless(begin_verse_id=begin_verse_id, \n",
    "                                                   end_verse_id=end_verse_id, \n",
    "                                                   end_poem_id=end_poem_id)\n",
    "trainer = build_trainer(model, tokenizer, datasets, 50, bs=bs, learning_rate=5e-5,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                        preprocess_logits_for_metrics=preprocess_logits_for_metadataless_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainer.args.num_train_epochs, trainer.args.max_steps, trainer.args.evaluation_strategy, trainer.args.eval_steps, trainer.args.save_strategy, \n",
    "trainer.args.learning_rate, trainer.args.lr_scheduler_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, best at 4 epochs', pwd='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr 2: train with the available label inserted as an additional verse at the beginning of the poem\n",
    "\n",
    "Some poems have only topic as label and the rest have only form as label, but here we train the same model for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False\n",
    "\n",
    "train_text_path1 = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_topic.train.{lang_str}.txt\"\n",
    "train_text_path2 = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form.train.{lang_str}.txt\"\n",
    "valid_text_path1 = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_topic.valid.{lang_str}.txt\"\n",
    "valid_text_path2 = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form.valid.{lang_str}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name = f\"gpt2-poems-cond-topic-or-form.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HUGGING_FACE_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen = HappyGeneration(\"GPT2\", orig_model_name)\n",
    "model = happy_gen.model\n",
    "tokenizer = happy_gen.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = PoemsFileConfig.from_json(Path(train_text_path1).parent/'all_poems.en.conf.json')\n",
    "begin_verse_id, end_verse_id, end_poem_id = setup_tokenizer(tokenizer, \n",
    "                                                            file_config, \n",
    "                                                            model)\n",
    "(tokenizer.additional_special_tokens, \n",
    " tokenizer.additional_special_tokens_ids, \n",
    " tokenizer.all_special_tokens, \n",
    " begin_verse_id, end_verse_id, end_poem_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_topic_labeled = get_datasets((train_text_path1, valid_text_path1), tokenizer, num_procs=1)\n",
    "datasets_form_labeled = get_datasets((train_text_path2, valid_text_path2), tokenizer, num_procs=1)\n",
    "datasets = DatasetDict({\n",
    "    split: concatenate_datasets((datasets_topic_labeled[split], datasets_form_labeled[split]))\n",
    "    for split in datasets_topic_labeled.keys()\n",
    "})\n",
    "datasets_topic_labeled, datasets_form_labeled, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "n_steps_by_epoch = len(datasets['train']) // bs\n",
    "compute_metrics = get_compute_metrics_metadataless(begin_verse_id=begin_verse_id, \n",
    "                                                   end_verse_id=end_verse_id, \n",
    "                                                   end_poem_id=end_poem_id,\n",
    "                                                   n_initial_verses_to_ignore=1)\n",
    "trainer = build_trainer(model, tokenizer, datasets, 50, bs=bs, learning_rate=5e-5,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                        preprocess_logits_for_metrics=preprocess_logits_for_metadataless_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 6 epochs', pwd='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr 3: train with labels inserted as additional verses at the beginning of the poem, '?' verse for labels not available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic.train.{lang_str}.txt\"\n",
    "valid_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic.valid.{lang_str}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name = f\"gpt2-poems-cond-form-and-topic.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HUGGING_FACE_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen = HappyGeneration(\"GPT2\", orig_model_name)\n",
    "model = happy_gen.model\n",
    "tokenizer = happy_gen.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = PoemsFileConfig.from_json(Path(train_text_path).parent/'all_poems.en.conf.json')\n",
    "begin_verse_id, end_verse_id, end_poem_id = setup_tokenizer(tokenizer, \n",
    "                                                            file_config, \n",
    "                                                            model)\n",
    "(tokenizer.additional_special_tokens, \n",
    " tokenizer.additional_special_tokens_ids, \n",
    " tokenizer.all_special_tokens, \n",
    " begin_verse_id, end_verse_id, end_poem_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets((train_text_path, valid_text_path), tokenizer, num_procs=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "n_steps_by_epoch = len(datasets['train']) // bs\n",
    "compute_metrics = get_compute_metrics_metadataless(begin_verse_id=begin_verse_id, \n",
    "                                                   end_verse_id=end_verse_id, \n",
    "                                                   end_poem_id=end_poem_id,\n",
    "                                                   n_initial_verses_to_ignore=2)\n",
    "trainer = build_trainer(model, tokenizer, datasets, 50, bs=bs, learning_rate=5e-5,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                        preprocess_logits_for_metrics=preprocess_logits_for_metadataless_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 3 epochs', pwd='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr 4: train with labels inserted as an additional verse at the beginning of the poem, with key: value format\n",
    "\n",
    "For instance:\n",
    "\n",
    "form: sonnet, topic: ?\\n<br>\n",
    "Verse 1\\n<br>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_kv.train.{lang_str}.txt\"\n",
    "valid_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_kv.valid.{lang_str}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name = f\"gpt2-poems-cond-form-and-topic-kv.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HUGGING_FACE_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen = HappyGeneration(\"GPT2\", custom_model_name)\n",
    "model = happy_gen.model\n",
    "tokenizer = happy_gen.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = PoemsFileConfig.from_json(Path(train_text_path).parent/'all_poems.en.conf.json')\n",
    "begin_verse_id, end_verse_id, end_poem_id = setup_tokenizer(tokenizer, \n",
    "                                                            file_config, \n",
    "                                                            model)\n",
    "tokenizer.all_special_tokens, begin_verse_id, end_verse_id, end_poem_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets((train_text_path, valid_text_path), tokenizer, num_procs=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "compute_metrics = get_compute_metrics_metadataless(begin_verse_id=begin_verse_id, \n",
    "                                                   end_verse_id=end_verse_id, \n",
    "                                                   end_poem_id=end_poem_id,\n",
    "                                                   n_initial_verses_to_ignore=1)\n",
    "trainer = build_trainer(model, tokenizer, datasets, 50, bs=bs, learning_rate=5e-5,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                        preprocess_logits_for_metrics=preprocess_logits_for_metadataless_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l checkpoints\n",
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Update model, 3 epochs (best valid score)', pwd='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr 5: train with a verse inserted for each label at the beginning of the poem, with key: value format\n",
    "\n",
    "For instance:\n",
    "\n",
    "form: sonnet \\n<br>\n",
    "topic: ? \\n <br>\n",
    "Verse 1 \\n <br>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_kv_mv.train.{lang_str}.txt\"\n",
    "valid_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_kv_mv.valid.{lang_str}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name = f\"gpt2-poems-cond-form-and-topic-kv-mv.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HUGGING_FACE_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen = HappyGeneration(\"GPT2\", orig_model_name)\n",
    "model = happy_gen.model\n",
    "tokenizer = happy_gen.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = PoemsFileConfig.from_json(Path(train_text_path).parent/'all_poems.en.conf.json')\n",
    "begin_verse_id, end_verse_id, end_poem_id = setup_tokenizer(tokenizer, \n",
    "                                                            file_config, \n",
    "                                                            model)\n",
    "tokenizer.all_special_tokens, begin_verse_id, end_verse_id, end_poem_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets((train_text_path, valid_text_path), tokenizer, num_procs=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "compute_metrics = get_compute_metrics_metadataless(begin_verse_id=begin_verse_id, \n",
    "                                                   end_verse_id=end_verse_id, \n",
    "                                                   end_poem_id=end_poem_id,\n",
    "                                                   n_initial_verses_to_ignore=2)\n",
    "trainer = build_trainer(model, tokenizer, datasets, 50, bs=bs, learning_rate=5e-5,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                        preprocess_logits_for_metrics=preprocess_logits_for_metadataless_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l checkpoints\n",
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 4 epochs', pwd='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr 6: train with a description of the labels at the beginning of the poem, '?' for labels not available\n",
    "\n",
    "For instance (assume form is not available):\n",
    "\n",
    "This is a poem with ? form about love: \\n<br>\n",
    "Verse 1 \\n <br>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_exp.train.{lang_str}.txt\"\n",
    "valid_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_exp.valid.{lang_str}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name = f\"gpt2-poems-cond-form-and-topic-exp.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HUGGING_FACE_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen = HappyGeneration(\"GPT2\", orig_model_name)\n",
    "model = happy_gen.model\n",
    "tokenizer = happy_gen.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = PoemsFileConfig.from_json(Path(train_text_path).parent/'all_poems.en.conf.json')\n",
    "begin_verse_id, end_verse_id, end_poem_id = setup_tokenizer(tokenizer, \n",
    "                                                            file_config, \n",
    "                                                            model)\n",
    "tokenizer.all_special_tokens, begin_verse_id, end_verse_id, end_poem_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets((train_text_path, valid_text_path), tokenizer, num_procs=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "compute_metrics = get_compute_metrics_metadataless(begin_verse_id=begin_verse_id, \n",
    "                                                   end_verse_id=end_verse_id, \n",
    "                                                   end_poem_id=end_poem_id,\n",
    "                                                   n_initial_verses_to_ignore=1)\n",
    "trainer = build_trainer(model, tokenizer, datasets, 50, bs=bs, learning_rate=5e-5,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                        preprocess_logits_for_metrics=preprocess_logits_for_metadataless_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l checkpoints\n",
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 3 epochs', pwd='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr 7: train with a description of the labels at the beginning of the poem, no description for categories not available\n",
    "\n",
    "For instance (assume form is not available):\n",
    "\n",
    "This is a poem about love: \\n<br>\n",
    "Verse 1 \\n <br>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_exp_s.train.{lang_str}.txt\"\n",
    "valid_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_exp_s.valid.{lang_str}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name = f\"gpt2-poems-cond-form-or-topic-exp-s.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HUGGING_FACE_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen = HappyGeneration(\"GPT2\", orig_model_name)\n",
    "model = happy_gen.model\n",
    "tokenizer = happy_gen.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = PoemsFileConfig.from_json(Path(train_text_path).parent/'all_poems.en.conf.json')\n",
    "begin_verse_id, end_verse_id, end_poem_id = setup_tokenizer(tokenizer, \n",
    "                                                            file_config, \n",
    "                                                            model)\n",
    "tokenizer.all_special_tokens, begin_verse_id, end_verse_id, end_poem_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets((train_text_path, valid_text_path), tokenizer, num_procs=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(datasets['train'][1000]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "compute_metrics = get_compute_metrics_metadataless(begin_verse_id=begin_verse_id, \n",
    "                                                   end_verse_id=end_verse_id, \n",
    "                                                   end_poem_id=end_poem_id,\n",
    "                                                   n_initial_verses_to_ignore=1)\n",
    "trainer = build_trainer(model, tokenizer, datasets, 50, bs=bs, learning_rate=5e-5,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                        preprocess_logits_for_metrics=preprocess_logits_for_metadataless_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l checkpoints\n",
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, './checkpoints/checkpoint-13545', #get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 3 epochs', pwd='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr 8: train with a description of the labels at the beginning of the poem, missing labels filled by classifiers\n",
    "\n",
    "For instance (assume form is not available):\n",
    "\n",
    "This is a poem with sonnet form about love: \\n<br>\n",
    "Verse 1 \\n <br>\n",
    "...\n",
    "\n",
    "The missing labels are filled with the labels predicted by classifiers of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_exp_filled.train.{lang_str}.txt\"\n",
    "valid_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_exp_filled.valid.{lang_str}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name = f\"gpt2-poems-cond-form-and-topic-exp-filled.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HUGGING_FACE_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen = HappyGeneration(\"GPT2\", orig_model_name)\n",
    "model = happy_gen.model\n",
    "tokenizer = happy_gen.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = PoemsFileConfig.from_json(Path(train_text_path).parent/'all_poems.en.conf.json')\n",
    "begin_verse_id, end_verse_id, end_poem_id = setup_tokenizer(tokenizer, \n",
    "                                                            file_config, \n",
    "                                                            model)\n",
    "tokenizer.all_special_tokens, begin_verse_id, end_verse_id, end_poem_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets((train_text_path, valid_text_path), tokenizer, num_procs=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(datasets['train'][1000]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "compute_metrics = get_compute_metrics_metadataless(begin_verse_id=begin_verse_id, \n",
    "                                                   end_verse_id=end_verse_id, \n",
    "                                                   end_poem_id=end_poem_id,\n",
    "                                                   n_initial_verses_to_ignore=1)\n",
    "trainer = build_trainer(model, tokenizer, datasets, 50, bs=bs, learning_rate=5e-5,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                        preprocess_logits_for_metrics=preprocess_logits_for_metadataless_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l checkpoints\n",
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 4 epochs', pwd='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr 9: train with a description of the labels at the beginning of the poem, '?' for labels not available, conditional loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:49.554428Z",
     "iopub.status.busy": "2022-04-04T21:37:49.554174Z",
     "iopub.status.idle": "2022-04-04T21:37:49.558748Z",
     "shell.execute_reply": "2022-04-04T21:37:49.557491Z",
     "shell.execute_reply.started": "2022-04-04T21:37:49.554389Z"
    }
   },
   "outputs": [],
   "source": [
    "resume_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:50.110954Z",
     "iopub.status.busy": "2022-04-04T21:37:50.110614Z",
     "iopub.status.idle": "2022-04-04T21:37:50.116188Z",
     "shell.execute_reply": "2022-04-04T21:37:50.114878Z",
     "shell.execute_reply.started": "2022-04-04T21:37:50.110920Z"
    }
   },
   "outputs": [],
   "source": [
    "train_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_exp.train.{lang_str}.txt\"\n",
    "valid_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.labeled.by_form_and_topic_exp.valid.{lang_str}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:50.531201Z",
     "iopub.status.busy": "2022-04-04T21:37:50.530453Z",
     "iopub.status.idle": "2022-04-04T21:37:50.536271Z",
     "shell.execute_reply": "2022-04-04T21:37:50.535025Z",
     "shell.execute_reply.started": "2022-04-04T21:37:50.531166Z"
    }
   },
   "outputs": [],
   "source": [
    "custom_model_name = f\"gpt2-poems-cond-form-and-topic-exp-condloss.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:50.978425Z",
     "iopub.status.busy": "2022-04-04T21:37:50.978120Z",
     "iopub.status.idle": "2022-04-04T21:37:50.984113Z",
     "shell.execute_reply": "2022-04-04T21:37:50.982720Z",
     "shell.execute_reply.started": "2022-04-04T21:37:50.978394Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HUGGING_FACE_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:52.204414Z",
     "iopub.status.busy": "2022-04-04T21:37:52.203837Z",
     "iopub.status.idle": "2022-04-04T21:37:59.891756Z",
     "shell.execute_reply": "2022-04-04T21:37:59.890775Z",
     "shell.execute_reply.started": "2022-04-04T21:37:52.204377Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(orig_model_name, use_cache=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(orig_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:37:59.894161Z",
     "iopub.status.busy": "2022-04-04T21:37:59.893872Z",
     "iopub.status.idle": "2022-04-04T21:38:00.608296Z",
     "shell.execute_reply": "2022-04-04T21:38:00.607119Z",
     "shell.execute_reply.started": "2022-04-04T21:37:59.894130Z"
    }
   },
   "outputs": [],
   "source": [
    "file_config = PoemsFileConfig.from_json(Path(train_text_path).parent/'all_poems.en.conf.json')\n",
    "begin_verse_id, end_verse_id, end_poem_id = setup_tokenizer(tokenizer, \n",
    "                                                            file_config, \n",
    "                                                            model)\n",
    "tokenizer.all_special_tokens, begin_verse_id, end_verse_id, end_poem_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:38:00.610929Z",
     "iopub.status.busy": "2022-04-04T21:38:00.610449Z",
     "iopub.status.idle": "2022-04-04T21:38:01.243408Z",
     "shell.execute_reply": "2022-04-04T21:38:01.242191Z",
     "shell.execute_reply.started": "2022-04-04T21:38:00.610849Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = get_datasets((train_text_path, valid_text_path), tokenizer, num_procs=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine an input example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:38:01.246493Z",
     "iopub.status.busy": "2022-04-04T21:38:01.246089Z",
     "iopub.status.idle": "2022-04-04T21:38:01.613558Z",
     "shell.execute_reply": "2022-04-04T21:38:01.612376Z",
     "shell.execute_reply.started": "2022-04-04T21:38:01.246447Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(datasets['train'][1000]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the classifiers needed for conditional loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:38:01.615996Z",
     "iopub.status.busy": "2022-04-04T21:38:01.615530Z",
     "iopub.status.idle": "2022-04-04T21:38:05.305180Z",
     "shell.execute_reply": "2022-04-04T21:38:05.303913Z",
     "shell.execute_reply.started": "2022-04-04T21:38:01.615934Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_by_topic_checkpoint = \"gpt2-poems-clf-by-topic.en\"\n",
    "clf_by_form_checkpoint  = \"gpt2-poems-clf-by-form.en\"\n",
    "hf_pwd = 'YOUR_HUGGING_FACE_PASSWORD'\n",
    "download_checkpoint_from_hf_hub(clf_by_topic_checkpoint, HF_USER, hf_pwd)\n",
    "download_checkpoint_from_hf_hub(clf_by_form_checkpoint, HF_USER, hf_pwd)\n",
    "hf_pwd = ''\n",
    "clf_by_topic = AutoModelForSequenceClassification.from_pretrained(clf_by_topic_checkpoint, use_cache=False)\n",
    "tokenizer_clf_by_topic = AutoTokenizer.from_pretrained(clf_by_topic_checkpoint)\n",
    "clf_by_form = AutoModelForSequenceClassification.from_pretrained(clf_by_form_checkpoint, use_cache=False)\n",
    "tokenizer_clf_by_form = AutoTokenizer.from_pretrained(clf_by_form_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:38:06.114265Z",
     "iopub.status.busy": "2022-04-04T21:38:06.113979Z",
     "iopub.status.idle": "2022-04-04T21:38:08.842872Z",
     "shell.execute_reply": "2022-04-04T21:38:08.841811Z",
     "shell.execute_reply.started": "2022-04-04T21:38:06.114234Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 1\n",
    "compute_metrics = get_compute_metrics_metadataless(begin_verse_id=begin_verse_id, \n",
    "                                                   end_verse_id=end_verse_id, \n",
    "                                                   end_poem_id=end_poem_id,\n",
    "                                                   n_initial_verses_to_ignore=1)\n",
    "\n",
    "labels_decoder = LabelsDecoderExplained()\n",
    "cond_loss_by_topic = ConditionalGenLoss(\n",
    "    clf_by_topic, tokenizer_clf_by_topic, tokenizer, LabelsType.Topics, labels_decoder, file_config, \n",
    "    end_poem_id, gen_bov_token_id=begin_verse_id, gen_eov_token_id=end_verse_id, device='cuda',\n",
    "    # This is needed because `GPT2ForSequenceClassification` doesn't know how to deal with padded\n",
    "    # inputs when `inputs_embeds` are passed instead of `input_ids`.\n",
    "    # It also helps with OOM issues.\n",
    "    max_clf_bs=1,\n",
    "    #max_clf_input_tokens=600\n",
    ")\n",
    "def cond_loss_by_topic_fn(output, labels):\n",
    "    return cond_loss_by_topic(output[:, :-1], labels[:, 1:])\n",
    "        \n",
    "\n",
    "cond_loss_by_form = ConditionalGenLoss(\n",
    "    clf_by_form, tokenizer_clf_by_form, tokenizer, LabelsType.Forms, labels_decoder, file_config, \n",
    "    end_poem_id, gen_bov_token_id=begin_verse_id, gen_eov_token_id=end_verse_id, device='cuda',\n",
    "    # This is needed because `GPT2ForSequenceClassification` doesn't know how to deal with padded\n",
    "    # inputs when `inputs_embeds` are passed instead of `input_ids`.\n",
    "    # It also helps with OOM issues.\n",
    "    max_clf_bs=1,\n",
    "    #max_clf_input_tokens=600,\n",
    ")\n",
    "def cond_loss_by_form_fn(output, labels):\n",
    "    return cond_loss_by_form(output[:, :-1], labels[:, 1:])\n",
    "\n",
    "\n",
    "extra_loss_fns = {\n",
    "    'topic cond loss': (cond_loss_by_topic_fn, 1.),\n",
    "    'form cond loss': (cond_loss_by_form_fn, 1.),\n",
    "}\n",
    "\n",
    "\n",
    "trainer = build_trainer(model, tokenizer, datasets, 50, bs=bs, learning_rate=5e-5,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                        preprocess_logits_for_metrics=preprocess_logits_for_metadataless_loss,\n",
    "                        extra_loss_fns=extra_loss_fns,\n",
    "                        #fp16=True, \n",
    "                        gradient_checkpointing=True,\n",
    "                        #optim='adafactor'\n",
    "                        adafactor=True, # deprecated soon, change to above when HF version > 4.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:38:08.845100Z",
     "iopub.status.busy": "2022-04-04T21:38:08.844757Z",
     "iopub.status.idle": "2022-04-04T21:38:08.851318Z",
     "shell.execute_reply": "2022-04-04T21:38:08.850020Z",
     "shell.execute_reply.started": "2022-04-04T21:38:08.845057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Temporarily disable GPT2ForSequenceClassification warning for its inability\n",
    "# to handle padding\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T21:38:08.990062Z",
     "iopub.status.busy": "2022-04-04T21:38:08.989712Z"
    }
   },
   "outputs": [],
   "source": [
    "# with torch.profiler.profile(\n",
    "#         schedule=torch.profiler.schedule(wait=0, warmup=0, active=4, repeat=10),\n",
    "#         #schedule=torch.profiler.schedule(wait=0, warmup=1, active=3, repeat=10),\n",
    "#         on_trace_ready=torch.profiler.tensorboard_trace_handler('./log'),\n",
    "#         record_shapes=True,\n",
    "#         profile_memory=True,\n",
    "#         with_stack=True\n",
    "# ) as prof:\n",
    "# To save profiling info, execute `prof.step()` inside some callback\n",
    "# The Tensorboard Pytorch Profiler is able to load the generated logs\n",
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l checkpoints\n",
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 4 epochs', pwd='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
