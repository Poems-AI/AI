{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File structures comparison\n",
    "\n",
    "This notebook can be used to compare different formats of the input data, like verse tagging, how are the poems grouped into sequences, ..., by looking at the validation loss.\n",
    "\n",
    "Set `run_as_standalone_nb = True` if you are running this notebook outside of a clone of its repository (https://github.com/Poems-AI/AI.git). For example, in a Colab or Kaggle notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_as_standalone_nb = False\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "if run_as_standalone_nb:\n",
    "    import sys    \n",
    "    root_lib_path = Path('AI').resolve()\n",
    "    if not root_lib_path.exists():\n",
    "        !git clone https://github.com/Poems-AI/AI.git\n",
    "    if str(root_lib_path) not in sys.path:\n",
    "        sys.path.insert(0, str(root_lib_path))\n",
    "    \n",
    "    !pip install -r {root_lib_path/'requirements.txt'}\n",
    "    !apt-get install git-lfs\n",
    "    !git lfs install\n",
    "else:\n",
    "    import local_lib_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6y8saYIIMjj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from enum import auto, Enum\n",
    "from functools import partial\n",
    "from happytransformer import fine_tuning_util, HappyGeneration\n",
    "from huggingface_hub import login, notebook_login\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import transformers\n",
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "from transformers.optimization import SchedulerType\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poemsai.data import Lang, lang_to_str, PoemsFileConfig\n",
    "from poemsai.hf_utils import model_to_url\n",
    "from poemsai.metrics import (compute_lm_accuracy, get_compute_metrics_metadataless, MetadataLessLoss, \n",
    "                             preprocess_logits_for_accuracy, preprocess_logits_for_metadataless_loss)\n",
    "from poemsai.nb_utils import commit_checkpoint_to_hf_hub, download_checkpoint_from_hf_hub\n",
    "from poemsai.tokenization import add_special_token\n",
    "from poemsai.trainer import PoemsTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone our datasets repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Poems-AI/dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log in to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_USER = 'YOUR_HF_USER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1: notebook_login.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: get token.** Unfortunately, you need to manually set your password. Every time you push to hub, you'll need to pass `use_auth_token=login_token`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = 'YOUR_HF_PASSWORD'\n",
    "login_token = login(HF_USER, pwd)\n",
    "pwd = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3 (recommended): interact with the git repo that stores your model** and pass the password every time you commit\n",
    "<br><br>\n",
    "Before commiting, you need to tell git your user and email (from HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_EMAIL = \"YOUR_HF_EMAIL\"\n",
    "!git config --global user.email $HF_EMAIL\n",
    "!git config --global user.name $HF_USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can push to hub by calling `commit_checkpoint_to_hub`. For instance:\n",
    "```\n",
    "commit_checkpoint_to_hub('gpt2-poems.en', HF_USER, './checkpoints/checkpoint-7170', \n",
    "                         message='Update model after 50 epochs', pwd='your_hf_password')\n",
    "```\n",
    "Be aware that this will copy everything from the checkpoint to the repository. If you need more control,\n",
    "clone the model repository and interact with it like with any other GitHub repository. You can get the url of the\n",
    "model repository with `model_to_url`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the language of the poems generator (`Lang.English` or `Lang.Spanish`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = Lang.English\n",
    "lang_str = lang_to_str(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(txt_paths, tokenizer, num_procs=1):\n",
    "    train_path, eval_path = txt_paths\n",
    "    dataset = load_dataset(\"text\", data_files={\"train\": train_path, \"eval\": eval_path})\n",
    "    tokenized_dataset = fine_tuning_util.preprocess_concatenate(tokenizer, dataset, num_procs, mlm=False)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer(tokenizer, file_config, model):\n",
    "    bov_token_id, eov_token_id, eop_token_id = None, None, None\n",
    "    \n",
    "    if file_config.beginning_of_verse_token != '':\n",
    "        bov_token_id = add_special_token(file_config.beginning_of_verse_token,\n",
    "                                         tokenizer,\n",
    "                                         model,\n",
    "                                         copy_from='')    \n",
    "    if file_config.end_of_verse_token != '':\n",
    "        eov_token_id = add_special_token(file_config.end_of_verse_token,\n",
    "                                         tokenizer,\n",
    "                                         model,\n",
    "                                         copy_from='\\n')\n",
    "    if file_config.end_of_poem_token != '':\n",
    "        eop_token_id = add_special_token(file_config.end_of_poem_token,\n",
    "                                         tokenizer,\n",
    "                                         model,\n",
    "                                         copy_from=tokenizer.eos_token)\n",
    "        \n",
    "    return bov_token_id, eov_token_id, eop_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(params):\n",
    "    for p in params: p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory to store the checkpoints automatically saved by the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trainer(model, tokenizer, datasets, n_epochs, bs=1, output_path='./checkpoints', optimizers=(None, None),\n",
    "                  compute_metrics=None, preprocess_logits_for_metrics=None, **train_kwargs):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_path,\n",
    "        evaluation_strategy=transformers.trainer_utils.IntervalStrategy.EPOCH,\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs,\n",
    "        num_train_epochs=n_epochs,\n",
    "        save_strategy=transformers.trainer_utils.IntervalStrategy.EPOCH,\n",
    "        report_to=[\"none\"],\n",
    "        fp16=False,\n",
    "        **train_kwargs\n",
    "    )\n",
    "    trainer = PoemsTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets['train'],\n",
    "        eval_dataset=datasets['eval'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "        optimizers=optimizers,\n",
    "        compute_metrics=compute_metrics,\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TR 1: simple file structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only extra characters present in the inputs are line breaks at the end of each verse.\n",
    "\n",
    "Set `resume_training = True` to continue training your own model from a previous checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = f\"dataset/all.txt/{lang_str}.txt/simple/all_poems.train.{lang_str}.txt\"\n",
    "valid_text_path = f\"dataset/all.txt/{lang_str}.txt/simple/all_poems.valid.{lang_str}.txt\"\n",
    "custom_model_name = f\"gpt2-poems-simple.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HF_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen = HappyGeneration(\"GPT2\", orig_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets((train_text_path, valid_text_path), happy_gen.tokenizer, num_procs=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TR 1a: all layers unfrozen since the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "bs = 1\n",
    "n_steps_by_epoch = len(datasets['train']) // bs\n",
    "trainer = build_trainer(happy_gen.model, happy_gen.tokenizer, datasets, n_epochs, bs=bs, learning_rate=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainer.args.num_train_epochs, trainer.args.max_steps, trainer.args.evaluation_strategy, trainer.args.eval_steps, trainer.args.save_strategy, \n",
    "trainer.args.learning_rate, trainer.args.lr_scheduler_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to set your Hugging Face password before commiting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 50 epochs', pwd='YOUR_HF_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy_gen.tokenizer.push_to_hub(model_url, use_auth_token=login_token)\n",
    "# happy_gen.model.push_to_hub(model_url, use_auth_token=login_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tr 1b: everything frozen but head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "bs = 1\n",
    "n_steps_by_epoch = len(datasets['train']) // bs\n",
    "trainer = build_trainer(happy_gen.model, happy_gen.tokenizer, datasets, n_epochs, bs=bs, learning_rate=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainer.args.num_train_epochs, trainer.args.max_steps, trainer.args.evaluation_strategy, trainer.args.eval_steps, \n",
    " trainer.args.save_strategy, trainer.args.learning_rate, trainer.args.lr_scheduler_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(happy_gen.model.parameters())), sum([p.requires_grad for p in happy_gen.model.parameters()])\n",
    "# > 148, 148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights of input embedding and head linear layer are tied, so if we train the head we are forced to train the embedding too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(happy_gen.model.transformer.wte.weight.shape, happy_gen.model.lm_head.weight.shape, \n",
    " id(happy_gen.model.transformer.wte.weight) - id(happy_gen.model.lm_head.weight),\n",
    " torch.allclose(happy_gen.model.transformer.wte.weight, happy_gen.model.lm_head.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we freeze everything but head and input embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze(p for p in happy_gen.model.parameters() if id(p) != id(happy_gen.model.lm_head.weight))\n",
    "sum([p.requires_grad for p in happy_gen.model.parameters()]), happy_gen.model.lm_head.weight.requires_grad\n",
    "# Expected output: (1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 50 epochs', pwd='YOUR_HF_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy_gen.tokenizer.push_to_hub(model_url, use_auth_token=login_token)\n",
    "# happy_gen.model.push_to_hub(model_url, use_auth_token=login_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tr 1c: discriminative learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact the input embedding is tied to the output linear layer probably makes this strategy less interesting, given that:\n",
    "- The final linear layer needs to have one of the highest learning rates. \n",
    "- If this is the case, the input embedding has a relatively high learning rate too (the same as the linear layer).\n",
    "- Then, it doesn't make sense for the early layers to learn slowly when the layer that feeds them is changing quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opt_disc_lrs(model, min_lr, max_lr, head_lr):\n",
    "    n_blocks = len(happy_gen.model.transformer.h)\n",
    "    lr_mult = (max_lr / min_lr) ** (1 / (n_blocks - 1))\n",
    "    blocks_lrs = [min_lr * lr_mult ** i for i in range(n_blocks)]\n",
    "    blocks_params = [{'params': happy_gen.model.transformer.h[i].parameters(), 'lr': blocks_lrs[i]}\n",
    "                     for i in range(n_blocks)]\n",
    "    return torch.optim.AdamW([\n",
    "        {'params': model.transformer.wpe.parameters(), 'lr': min_lr},\n",
    "        {'params': model.transformer.ln_f.parameters(), 'lr': min_lr},\n",
    "        {'params': model.lm_head.parameters()},#, 'lr': head_lr},\n",
    "        *blocks_params\n",
    "    ], lr=head_lr, weight_decay=0, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "bs = 1\n",
    "n_steps_by_epoch = len(datasets['train']) // bs\n",
    "opt = create_opt_disc_lrs(happy_gen.model, 1e-7, 1e-4, 1e-4)\n",
    "#sched = transformers.get_constant_schedule_with_warmup(opt, n_steps_by_epoch//2, last_epoch=-1)\n",
    "trainer = build_trainer(happy_gen.model, happy_gen.tokenizer, datasets, n_epochs, bs=bs, optimizers=(opt, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 50 epochs', pwd='YOUR_HF_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy_gen.tokenizer.push_to_hub(model_url, use_auth_token=login_token)\n",
    "# happy_gen.model.push_to_hub(model_url, use_auth_token=login_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr 2: tag end of poems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only special tags are line breaks at the end of each verse and \"<|endoftext|>\" at the end of each poem.\n",
    "\n",
    "Set `resume_training = True` to continue training your own model from a previous checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.train.{lang_str}.txt\"\n",
    "valid_text_path = f\"dataset/all.txt/{lang_str}.txt/only_end_tags/all_poems.valid.{lang_str}.txt\"\n",
    "custom_model_name = f\"gpt2-poems-endtags.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HF_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen = HappyGeneration(\"GPT2\", orig_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = PoemsFileConfig.from_json(Path(train_text_path).parent/'all_poems.en.conf.json')\n",
    "begin_verse_id, end_verse_id, end_poem_id = setup_tokenizer(happy_gen.tokenizer, \n",
    "                                                            file_config, \n",
    "                                                            happy_gen.model)\n",
    "\n",
    "our_special_tokens = (file_config.end_of_verse_token,\n",
    "                      file_config.beginning_of_verse_token,\n",
    "                      file_config.end_of_poem_token,)\n",
    "our_special_tokens = [t for t in our_special_tokens if t != '']\n",
    "\n",
    "assert set(our_special_tokens).issubset(happy_gen.tokenizer.all_special_tokens), (\n",
    "       f'{our_special_tokens} != {happy_gen.tokenizer.additional_special_tokens}'\n",
    ")\n",
    "assert len(happy_gen.tokenizer.encode(''.join(our_special_tokens))) == len(our_special_tokens)\n",
    "(happy_gen.tokenizer.additional_special_tokens, \n",
    " happy_gen.tokenizer.additional_special_tokens_ids, \n",
    " happy_gen.tokenizer.all_special_tokens, \n",
    " begin_verse_id, end_verse_id, end_poem_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets((train_text_path, valid_text_path), happy_gen.tokenizer, num_procs=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of the decoding of a an already tokenized sample. If everything is ok, '\\\\n' should appear at the end of each verse and '<|endoftext|>' at the end of each poem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen.tokenizer.decode(datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation we'll use a modified version of the cross entropy loss that doesn't take into account the metadata inserted into the poems, like the end of poem tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "n_steps_by_epoch = len(datasets['train']) // bs\n",
    "compute_metrics = get_compute_metrics_metadataless(begin_verse_id=begin_verse_id, \n",
    "                                                   end_verse_id=end_verse_id, \n",
    "                                                   end_poem_id=end_poem_id)\n",
    "trainer = build_trainer(happy_gen.model, happy_gen.tokenizer, datasets, 50, bs=bs, learning_rate=5e-5,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                        preprocess_logits_for_metrics=preprocess_logits_for_metadataless_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 50 epochs', pwd='YOUR_HF_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy_gen.tokenizer.push_to_hub(model_url, use_auth_token=login_token)\n",
    "# happy_gen.model.push_to_hub(model_url, use_auth_token=login_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tr 3: include all tags and end of previous verses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the input files includes:\n",
    "* Beginning of verse tag\n",
    "* End of verse tag\n",
    "* End of poem tag\n",
    "* The last word of each of the 4 previous verses is placed right before the beginning of verse tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = f\"dataset/all.txt/{lang_str}.txt/all_tags_4_prev_verses/all_poems.train.{lang_str}.txt\"\n",
    "valid_text_path = f\"dataset/all.txt/{lang_str}.txt/all_tags_4_prev_verses/all_poems.valid.{lang_str}.txt\"\n",
    "custom_model_name = f\"gpt2-poems-alltags-4prev-verses.{lang_str}\"\n",
    "model_url = model_to_url(custom_model_name, HF_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo of our model to commit there later\n",
    "if resume_training:\n",
    "    hf_pwd = 'YOUR_HF_PASSWORD'\n",
    "    download_checkpoint_from_hf_hub(custom_model_name, HF_USER, hf_pwd)\n",
    "    hf_pwd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen = HappyGeneration(\"GPT2\", orig_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = PoemsFileConfig.from_json(Path(train_text_path).parent/'all_poems.en.conf.json')\n",
    "begin_verse_id, end_verse_id, end_poem_id = setup_tokenizer(happy_gen.tokenizer, \n",
    "                                                            file_config, \n",
    "                                                            happy_gen.model)\n",
    "\n",
    "our_special_tokens = (file_config.end_of_verse_token,\n",
    "                      file_config.beginning_of_verse_token,\n",
    "                      file_config.end_of_poem_token,)\n",
    "our_special_tokens = [t for t in our_special_tokens if t != '']\n",
    "\n",
    "assert set(our_special_tokens).issubset(happy_gen.tokenizer.all_special_tokens), (\n",
    "       f'{our_special_tokens} != {happy_gen.tokenizer.additional_special_tokens}'\n",
    ")\n",
    "assert len(happy_gen.tokenizer.encode(''.join(our_special_tokens))) == len(our_special_tokens)\n",
    "(happy_gen.tokenizer.additional_special_tokens, \n",
    " happy_gen.tokenizer.additional_special_tokens_ids, \n",
    " happy_gen.tokenizer.all_special_tokens, \n",
    " begin_verse_id, end_verse_id, end_poem_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets((train_text_path, valid_text_path), happy_gen.tokenizer, num_procs=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of the decoding of a an already tokenized sample. If everything is ok:\n",
    "* '\\<bov\\>' should appear at the end of each verse \n",
    "* '\\\\\\n' should appear at the end of each verse \n",
    "* '<|endoftext|>' should appear at the end of each poem.\n",
    "* The termination of the 4 previous verses should appear after each '\\<bov\\>' token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_gen.tokenizer.decode(datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation we'll use a modified version of the cross entropy loss that doesn't take into account the metadata inserted into the poems, like the end of poem tag or the terminations of previous verses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "n_steps_by_epoch = len(datasets['train']) // bs\n",
    "compute_metrics = get_compute_metrics_metadataless(begin_verse_id=begin_verse_id, \n",
    "                                                   end_verse_id=end_verse_id, \n",
    "                                                   end_poem_id=end_poem_id)\n",
    "trainer = build_trainer(happy_gen.model, happy_gen.tokenizer, datasets, 50, bs=bs, learning_rate=5e-5,\n",
    "                        compute_metrics=compute_metrics,\n",
    "                        preprocess_logits_for_metrics=preprocess_logits_for_metadataless_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=custom_model_name if resume_training else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_name, get_last_checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_checkpoint_to_hf_hub(custom_model_name, HF_USER, get_last_checkpoint('./checkpoints'),\n",
    "                            message='Add model, 50 epochs', pwd='YOUR_HF_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happy_gen.tokenizer.push_to_hub(model_url, use_auth_token=login_token)\n",
    "# happy_gen.model.push_to_hub(model_url, use_auth_token=login_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
