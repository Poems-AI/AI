{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebooks gathers, splits and preprocess the data we are using to train our poems generator.\n\nAt the end, it generates two text files in the current directory:\n* A concatenation of all the poems of the training set, with name `all_poems.train.[lang].txt\n* A concatenation of all the poems of the validation set, with name `all_poems.valid.[lang].txt","metadata":{}},{"cell_type":"code","source":"from enum import auto, Enum\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport re\nfrom typing import List\n\n\nclass Lang(Enum):\n    English = \"en\"\n    Spanish = \"es\"\n\n\ndef lang_to_str(lang:Lang):\n    return lang.value","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Edit the cell below to choose the language you want to generate the .txt for:","metadata":{}},{"cell_type":"code","source":"lang = Lang.English","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data collection","metadata":{}},{"cell_type":"markdown","source":"If outside of Kaggle, you should set `KAGGLE_DS_ROOT` to the root folder that contains the Kaggle datasets you are using.\n\nWe are currently using:\n* https://github.com/Poems-AI/dataset/tree/main/marcos_de_la_fuente.txt/en.txt: english poems by our poet Marcos de la Fuente\n* https://github.com/Poems-AI/dataset/tree/main/marcos_de_la_fuente.txt/es.txt: spanish poems by our poet Marcos de la Fuente\n* https://www.kaggle.com/michaelarman/poemsdataset) as an external english poetry dataset\n* https://www.kaggle.com/andreamorgar/spanish-poetry-dataset) as an external spanish poetry dataset","metadata":{}},{"cell_type":"code","source":"KAGGLE_DS_ROOT = Path('/kaggle/input')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/Poems-AI/dataset.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataSource(Enum):\n    Marcos = auto()\n    Kaggle = auto()\n\n\ndef get_ds_path(lang:Lang, source:DataSource):\n    assert source in (DataSource.Marcos, DataSource.Kaggle), 'Not implemented for given DataSource'\n    if source == DataSource.Marcos:\n        return Path(f'dataset/marcos_de_la_fuente.txt/{lang_to_str(lang)}.txt')\n    elif source == DataSource.Kaggle:\n        folder_name = 'poemsdataset' if lang == Lang.English else 'spanish-poetry-dataset'\n        return KAGGLE_DS_ROOT/folder_name\n\n\ndef get_text_files(path:Path):\n    result = []\n    if not path.is_dir(): return result\n    \n    for child in path.iterdir():\n        if child.is_dir():\n            result.extend(get_text_files(child))\n        elif (child.suffix.lower() == '.txt'):\n            result.append(child)\n    return result\n\n    \nclass PoemsFileList:\n    def __init__(self, paths:List[Path]):\n        self.paths = paths\n        \n    def __iter__(self):\n        return iter(self.paths)\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    @classmethod\n    def from_root_path(cls, root_path:Path, poem_titles_to_ignore:List[str]=None):\n        if poem_titles_to_ignore is None: poem_titles_to_ignore = []\n        paths = [p for p in get_text_files(root_path) if p.name not in poem_titles_to_ignore]\n        return cls(paths)\n\n\nclass PoemsDf:\n    def __init__(self, df, poems_column):\n        self.df = df\n        self.poems_column = poems_column\n    \n    def __len__(self):\n        return len(self.df)\n    \n    @classmethod\n    def from_csv_path(cls, csv_path, poems_column):\n        df = pd.read_csv(csv_path)\n        return cls(df, poems_column)\n    \n    \ndef get_data(lang:Lang, source:DataSource):\n    path = get_ds_path(lang, source)\n    \n    # TODO: maybe the type could be inferred from the path, but it'd be just a heuristic at best\n    # It might be better to return csv directly from get_ds_path\n    if lang == Lang.Spanish:\n        if source == DataSource.Marcos:\n            poems_to_ignore = ['other_authors.es.txt']\n            return PoemsFileList.from_root_path(path, poem_titles_to_ignore=poems_to_ignore)\n        else:\n            return PoemsDf.from_csv_path(path/'poems.csv', 'content')\n    elif lang == Lang.English:\n        return PoemsFileList.from_root_path(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_sources = [get_data(lang, ds_type) for ds_type in DataSource]\n[(type(ds), len(ds)) for ds in data_sources]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split into training and validation set","metadata":{}},{"cell_type":"markdown","source":"Set the percentage of data to be used as validation set, given as a fraction of unity:","metadata":{}},{"cell_type":"code","source":"valid_pct = 0.2\nRNG_SEED = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PoemsFileListSplitterByParent:\n    #def __init__(self, poems_list:PoemsFileList):\n    #    self.poems_list = poems_list\n        \n    def split(self, poems_list:PoemsFileList, valid_pct=0.2):\n        all_train_files, all_valid_files = [], []\n        parent_paths = set(fp.parent for fp in poems_list)\n        poems_set = set(poems_list)\n        rng = np.random.default_rng(seed=RNG_SEED)\n\n        for parent in parent_paths:\n            # For any directory with at least two children, we choose at least \n            # one file for validation set, independently of valid_pct\n            txt_files_in_parent = set(get_text_files(parent)).intersection(poems_set)# , poems_list.poem_titles_to_ignore)\n            num_valid = (0 if len(txt_files_in_parent) <= 1 \n                         else max(1, round(valid_pct * len(txt_files_in_parent))))\n            valid_files_idxs = rng.choice(len(txt_files_in_parent), size=num_valid, replace=False)\n            files_arr = np.array(list(txt_files_in_parent))\n            valid_files = files_arr[valid_files_idxs]\n            train_files = txt_files_in_parent - set(valid_files)\n            all_valid_files.extend(valid_files)\n            all_train_files.extend(train_files)\n            #print(f'Added {len(valid_files)} valid, {len(train_files)} train, ratio: {len(valid_files)/(len(txt_files_in_parent))}')\n        \n        return PoemsFileList(all_train_files), PoemsFileList(all_valid_files)\n    \n    \nclass PoemsDfSplitter:\n    #def __init__(self, poems_list:PoemsDf):\n    #    self.poems_list = poems_list\n        \n    def split(self, poems_list:PoemsDf, valid_pct=0.2):\n        df = poems_list.df\n        all_idxs = list(range(len(df)))\n        num_valid = round(valid_pct * len(all_idxs))\n        rng = np.random.default_rng(seed=RNG_SEED)\n        valid_idxs = rng.choice(len(all_idxs), size=num_valid, replace=False)\n        train_idxs = list(set(all_idxs) - set(valid_idxs))\n        train_rows = df.iloc[train_idxs]\n        valid_rows = df.iloc[valid_idxs]\n        return PoemsDf(train_rows, poems_list.poems_column), PoemsDf(valid_rows, poems_list.poems_column)\n\n    \nclass SplitterFactory:\n    def get_splitter_for(self, data):\n        if isinstance(data, PoemsFileList):\n            return PoemsFileListSplitterByParent()\n        elif isinstance(data, PoemsDf):\n            return PoemsDfSplitter()\n        return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, valid_data = [], []\nsplitter_factory = SplitterFactory()\n\n# for ds_type in DataSource:\n#     data_source = get_data(lang, ds_type)\nfor data_source in data_sources:\n    splitter = splitter_factory.get_splitter_for(data_source)\n    train_data_source, valid_data_source = splitter.split(data_source, valid_pct)\n    train_data.append(train_data_source)\n    valid_data.append(valid_data_source)\n    \nsum(len(ds) for ds in train_data), sum(len(ds) for ds in valid_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read and merge poems by split","metadata":{}},{"cell_type":"code","source":"class PoemsDfReader():\n    def __init__(self, poems_df:PoemsDf):\n        self.poems_df = poems_df\n    \n    def __iter__(self):\n        poem_getter = lambda row: row[1][self.poems_df.poems_column]\n        df = self.poems_df.df\n        return (poem_getter(row).split('\\n') for row in df.iterrows() if isinstance(poem_getter(row), str))\n\n    \nclass PoemsFileReader():\n    def __init__(self, poems_list:PoemsFileList):\n        self.poems_list = poems_list\n    \n    def __iter__(self):\n        return FilesIterator(list(self.poems_list))\n    \n    \nclass FilesIterator():\n    def __init__(self, paths):\n        self.paths = paths\n        self.idx = 0\n        \n    def __next__(self):\n        if self.idx < len(self.paths):\n            path = self.paths[self.idx]\n            self.idx += 1\n            with open(path, 'r') as f:\n                text = f.readlines()\n            return text    \n        raise StopIteration \n\n        \nclass ComposedPoemsReader:\n    def __init__(self, readers):\n        self.readers = readers\n        \n    def __iter__(self):\n        return (poem for reader in self.readers for poem in reader)\n\n\nclass ReaderFactory:\n    def get_reader_for(self, data):\n        if isinstance(data, PoemsFileList):\n            return PoemsFileReader(data)\n        elif isinstance(data, PoemsDf):\n            return PoemsDfReader(data)\n        return None\n\n\nclass PoemsWriter():\n    def __init__(self, open_file, drop_multispace=False):\n        self.meaningful_chars_pattern = re.compile(\"[a-zA-Z0-9]\")\n        self.multispace_pattern = re.compile(\" {2,}\") if drop_multispace else None\n        self.file = open_file\n        \n    def write_verse(self, verse, endofpoem=False):\n        if self.meaningful_chars_pattern.search(verse) is None:\n            return\n        if self.multispace_pattern is not None:\n            verse = self.multispace_pattern.sub(\" \", verse)\n        verse = verse.strip()\n        if len(verse) > 0:\n            line_end = \"<endofpoem>\\n\" if endofpoem else \"\\\\n\\n\"\n            self.file.write(f'{verse} {line_end}')\n\n                    \ndef merge_poems(poems_reader, poems_writer):\n    for poem_lines in poems_reader:\n        #if not isinstance(poem_lines, list) or len(poem_lines) == 0:\n        #    continue\n        for line in poem_lines[:-1]:\n            poems_writer.write_verse(line)\n        if len(poem_lines) > 0:\n            poems_writer.write_verse(poem_lines[-1], endofpoem=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reader_factory = ReaderFactory()\ntrain_data_readers = [reader_factory.get_reader_for(data) for data in train_data]\nvalid_data_readers = [reader_factory.get_reader_for(data) for data in valid_data]\n\ntrain_txt_path = Path(f'./all_poems.train.{lang_to_str(lang)}.txt')\nvalid_txt_path = Path(f'./all_poems.valid.{lang_to_str(lang)}.txt')\nwith open(train_txt_path, \"w\") as train_txt_f:\n    merge_poems(ComposedPoemsReader(train_data_readers), PoemsWriter(train_txt_f))\nwith open(valid_txt_path, \"w\") as valid_txt_f:\n    merge_poems(ComposedPoemsReader(valid_data_readers), PoemsWriter(valid_txt_f))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show the number of lines by file:","metadata":{}},{"cell_type":"code","source":"!wc -l $train_txt_path\n!wc -l $valid_txt_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}